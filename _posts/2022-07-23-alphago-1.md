---
title: 论文笔记：AlphaGo 1st version
tags: AlphaGo Nature 论文 笔记
---

> 关于 AlphaGo 主要有以下四篇论文，依次代表了其的产生以及三次迭代：
>
> 1. [Mastering the game of Go with deep neural networks and tree search](https://www.nature.com/articles/nature16961);
> 2. [Mastering the game of Go without human knowledge](https://www.nature.com/articles/nature24270);
> 3. [Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/abs/1712.01815);
> 4. [Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model](https://arxiv.org/abs/1911.08265).
>
> 我会按照它们发布的顺序精读并将笔记发布到博客上，你可以通过 AlphaGo tag 检索到这些文章。
>
> 原文：Mastering the game of Go with deep neural networks and tree search

## 摘要

围棋一直以来都被认为是 AI 领域最有挑战的传统游戏，它的局势数量以及决策难度使其异常难以处理。这里本文提出了一种利用“价值网络”（Value Networks）来评估棋盘局势，利用“策略网络”（Policy Networks）来选择下一步的计算机下棋方法。其中的深度神经网络是通过利用人类专家游戏的监督学习，以及自我游戏的强化学习相结合的方式训练的。神经网络在没有采取任何 lookahead search 的情况下以 state-of-the-art（当时）的蒙特卡洛树搜索级别模拟成千上万的自我游戏。除此之外，本文还引入了一种新的搜索算法，它将蒙特卡洛模拟与“价值”和“策略网络”相结合。使用这种搜索算法的 AlphaGo 程序面对其他围棋程序的胜率达到了 99.8%，并以 5 比 0 战胜了人类欧洲围棋冠军。

## 简介

任何博弈游戏都有一个最优价值函数，$v^*(s)$，其决定了在所有玩家都满足完美游戏的情况下，经过每个 board position 或者说状态 $s$ 后的游戏结果。这类游戏通常可以通过递归计算搜索树中的最优价值函数来解决，这些搜索树大约存在 $b^d$ 个操作序列（其中 $b$ 是每个状态下可选操作的数量，而 $d$ 是游戏的“深度”，即回合次数。当面对规模较大的游戏时（国际象棋：$b≈35$, $d≈80$，围棋：$b≈250$, $d≈150$），穷举法显然不可取。但是有效搜索空间可以通过两个一般原则来减少：

1. 首先，搜索的深度 $d$ 可以通过每步评估来减少，每一步都截断搜索树，并默认每个子树的价值函数值都等于最优函数值（$v(s)=v^*(s)$）。这样就可以预测状态 $s$ 的结果。
2. 其次，搜索的广度 $b$ 可以通过采取特定策略 $p(a\|s)$ 来减少，其代表在状态 $s$ 的情况下采取动作 $a$ 的概率分布。例如，蒙特卡洛 rollouts 通过每次按照策略 $p$ 来采样这些长 action 序列，在完全不分支的情况下搜索到最大深度。将每次 rollout 的结果平均，能够提供一个非常有效的当前状态的评估。

> 在 AlphaGo 第一代的程序中，蒙特卡洛树搜索算法扮演了相当重要的角色。对这方面不太熟悉的同学可以看看以下两个视频学习一下：
>
> - [蒙特卡洛（Monte Carlo, MCMC）方法的原理和应用_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV17D4y1o7J2?spm_id_from=333.337.search-card.all.click&vd_source=f30820c70ea5bafca32297b879508ee9)
> - [Monte Carlo Tree Search - YouTube](https://www.youtube.com/watch?v=UXW2yZndl7U&t=240s)
>
> 简而言之，蒙特卡洛思想就是随机抽样的思想。当目标分布不可求，或者说计算成本不可接受的情况下，可以通过计算机模拟抽样的方式来估计目标分布。而蒙特卡洛树搜索则将每个状态 $s$ 表示为树的结点，通过每次从 $s$ 出发随机采样到最低深度后获得的最终价值来估计 $f(s)$ 的真实值。具体过程请读者自行查阅。

蒙特卡洛树搜索执行的模拟越多，其估计值更准确。随着估计值越来越准确，策略 $p$ 也随着时间的推移而改进。渐进地，该策略能够收敛到最优策略，而每个估计也会收敛到最优价值函数。第一代 AlphaGo 推出之前，最好的围棋程序就是基于 MCTS (Monte Carlo tree search)，并通过使用「预测人类专家动作」的策略来增强。这些策略用于缩小下一步动作到更高概率的选项，以及在 rollout 过程中的随机采样。AlphaGo 第一代在其基础上增加了 CNN 来表征 Position 信息，以此来减少蒙特卡洛树搜索的广度和深度。

### 训练

AlphaGo 第一代由包含了以下过程的流水线方式训练：

1. 只利用人类专家数据的监督学习——得到策略网络 $p_\sigma$；
2. 通过自我游戏训练的强化学习——调整监督学习的结果，使其更偏向于赢得游戏，而非重现训练结果；
3. 训练用于预测赢家的价值网络 $v_\theta$。

> ![]({{ site.url }}\assets\images\2022-07-23\Snipaste_2022-07-23_22-06-08.png){:height="300px" .border}
>
> 图 1：训练流水线的示意图


## 监督学习——“策略网络”

流水线的第一阶段的训练只把目标设为预测专家的操作。使用的神经网络也较为原始，网络仅包含卷积层和 rectifier nonlinearities 之间交叉，总层数也只有 13 层（具体网络结构论文中没有提及），最后通过 softmax 层输出下一步的概率分布。优化过程通过随机梯度上升来最大化预测下一步的似然。

$$\Delta\sigma\propto\frac{\partial\log{p_\sigma (a|s)}}{\partial\sigma}$$

这个网络的预测准确率能达到 57.0%，若使用最直观的输入：Position 信息以及回合历史，能达到 55.7%。

> 本文中有提到一个小而快的基于 rollout 的策略网络，它只能达到 24.2% 的准确率。由于我不太明白这个网络的研究意义是什么（后续的训练以及预测并不使用这个网络的结果），所以下文不做其他补充说明。

## 强化学习——依然是“策略网络”

强化学习对应的策略网络 $p_\rho$ 结构上和 $p_\sigma$ 完全一致，并且权重集 $\rho$ 也初始化为 $\sigma$ 训练完成后的值。训练过程中，模拟一方使用策略网络 $p_\rho$，另一方从先前迭代过程中的策略网络中随机选择一个，然后相互博弈。据文中描述这样的训练方式可以避免过拟合到当前策略上。文中使用的奖励函数在所有非结局 state 上均为 0。结局时，胜者奖励 +1，败者奖励 -1，在这之前，在每个 state 都使用梯度上升来更新权重。

在利用强化学习优化过权重之后，策略网络在对阵其他围棋程序时的胜率有了彻底的提升。

## 强化学习——“价值网络”

最后的训练的目标在于做到评估任意回合的 position information。价值网络的结构依然是和策略网络一样的，但是输出层不再是整个概率分布，而是单独一个输出。我的理解是，价值网络输入当下的棋局特征，并给出双方胜率的估计。训练集从之前的 $(s,\ a)$ 变成了 $(s,\ z)$。

在使用现成训练集训练价值网络时，由于 CNN 的特点是全局共享参数，而围棋的胜负更多的依赖上下文之间的联系。最终造成了 overfitting 的问题。作者解决这个问题的方法是暴力扩展数据集（3000 万个自我游戏过程），削弱单个游戏的占比，以此来提升模型的泛用性。

## 搜索树 + 两个网络

待续

## 引用

1. [Mastering the game of Go with deep neural networks and tree search - Nature](https://www.nature.com/articles/nature16961)
2. [蒙特卡洛（Monte Carlo, MCMC）方法的原理和应用_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV17D4y1o7J2?spm_id_from=333.337.search-card.all.click&vd_source=f30820c70ea5bafca32297b879508ee9)
3. [Monte Carlo Tree Search - YouTube](https://www.youtube.com/watch?v=UXW2yZndl7U&t=240s)











